{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSY340 Project - Trajectory Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill in group number and member names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"Bingcheng Chen\" \n",
    "NAME2 = \"Arvin Rokni\"\n",
    "GROUP = \"Project groups 64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model_cbc\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import dataloader\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model save location\n",
    "save_location = \"./models\"\n",
    "# defining dataset locations\n",
    "dataset_folder = \"./trajectory-prediction-transformers/datasets\"\n",
    "dataset_name = \"raw\"\n",
    "# setting validation size. if val_size = 0, split percentage is 80-20\n",
    "val_size = 0\n",
    "# length of sequence given to encoder\n",
    "gt = 8\n",
    "# length of sequence given to decoder\n",
    "horizon = 12\n",
    "\n",
    "\n",
    "train_dataset, _ = dataloader.create_dataset(dataset_folder, dataset_name, val_size, \\\n",
    "    gt, horizon, delim=\"\\t\", train=True)\n",
    "val_dataset, _ = dataloader.create_dataset(dataset_folder, dataset_name, val_size, \\\n",
    "    gt, horizon, delim=\"\\t\", train=False)\n",
    "# test_dataset, _ = dataloader.create_dataset(dataset_folder, dataset_name, val_size, \\\n",
    "#     gt, horizon, delim=\"\\t\", train=False, eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src': tensor([[ 4.9693e+00,  8.3395e+00,  0.0000e+00,  0.0000e+00],\n",
       "         [ 5.1330e+00,  8.3350e+00,  1.6374e-01, -4.5338e-03],\n",
       "         [ 5.2968e+00,  8.3307e+00,  1.6374e-01, -4.2963e-03],\n",
       "         [ 5.3915e+00,  8.3490e+00,  9.4709e-02,  1.8377e-02],\n",
       "         [ 5.4565e+00,  8.3774e+00,  6.5034e-02,  2.8400e-02],\n",
       "         [ 5.5218e+00,  8.4058e+00,  6.5244e-02,  2.8400e-02],\n",
       "         [ 5.5868e+00,  8.4342e+00,  6.5033e-02,  2.8400e-02],\n",
       "         [ 5.5988e+00,  8.4497e+00,  1.1997e-02,  1.5512e-02]]),\n",
       " 'trg': tensor([[ 5.5575e+00,  8.4521e+00, -4.1251e-02,  2.3870e-03],\n",
       "         [ 5.5163e+00,  8.4548e+00, -4.1251e-02,  2.6255e-03],\n",
       "         [ 5.4750e+00,  8.4571e+00, -4.1251e-02,  2.3861e-03],\n",
       "         [ 5.4340e+00,  8.4595e+00, -4.1041e-02,  2.3870e-03],\n",
       "         [ 5.3927e+00,  8.4622e+00, -4.1251e-02,  2.6245e-03],\n",
       "         [ 5.3515e+00,  8.4645e+00, -4.1251e-02,  2.3870e-03],\n",
       "         [ 5.3102e+00,  8.4669e+00, -4.1251e-02,  2.3861e-03],\n",
       "         [ 5.2690e+00,  8.4696e+00, -4.1251e-02,  2.6255e-03],\n",
       "         [ 5.2277e+00,  8.4719e+00, -4.1251e-02,  2.3870e-03],\n",
       "         [ 5.0924e+00,  8.4832e+00, -1.3533e-01,  1.1217e-02],\n",
       "         [ 4.9337e+00,  8.4968e+00, -1.5869e-01,  1.3603e-02],\n",
       "         [ 4.7748e+00,  8.5101e+00, -1.5890e-01,  1.3365e-02]]),\n",
       " 'frames': array([6130., 6140., 6150., 6160., 6170., 6180., 6190., 6200., 6210.,\n",
       "        6220., 6230., 6240., 6250., 6260., 6270., 6280., 6290., 6300.,\n",
       "        6310., 6320.]),\n",
       " 'seq_start': array([[4.9692917, 8.339489 ]], dtype=float32),\n",
       " 'dataset': 0,\n",
       " 'peds': 95.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset[10]['src'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining batch size\n",
    "batch_size = 64\n",
    "\n",
    "# creating torch dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_model = model_cbc.Transformer(encoder_input_size=4, decoder_input_size=4, embedding_size=512, num_heads=8, num_layers=6, feedforward_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Transformer(\n",
       "  (encoder_embedding): Embedding(4, 512)\n",
       "  (decoder_embedding): Embedding(4, 512)\n",
       "  (positional_encoding): PositionalEncoding()\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0-5): 6 x EncoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0-5): 6 x DecoderLayer(\n",
       "      (self_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (cross_attn): MultiHeadAttention(\n",
       "        (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "        (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (feed_forward): PositionWiseFeedForward(\n",
       "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "        (relu): ReLU()\n",
       "      )\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, optimizer, loss_fn, train_loader, val_loader, device, print_every):\n",
    "\n",
    "    model.train()\n",
    "    train_loss_batches = []\n",
    "    num_batches = len(train_loader)\n",
    "\n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "\n",
    "        encoder_input = data['src']\n",
    "        decoder_input = data['trg']\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        predictions = transformer_model.forward(encoder_input, decoder_input) # add mask ?\n",
    "        loss = loss_fn(predictions, decoder_input)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss_batches.append(loss.item())\n",
    "\n",
    "        # If you want to print your progress more often than every epoch you can\n",
    "        # set `print_every` to the number of batches you want between every status update.\n",
    "        # Note that the print out will trigger a full validation on the full val. set => slows down training\n",
    "        if print_every is not None and batch_index % print_every == 0:\n",
    "            val_loss = validate(model, loss_fn, val_loader, device)\n",
    "            model.train()\n",
    "            print(f\"\\tBatch {batch_index}/{num_batches}: \"\n",
    "                  f\"\\tTrain loss: {sum(train_loss_batches[-print_every:])/print_every:.3f}, \"\n",
    "                  f\"\\tVal. loss: {val_loss:.3f}, \")\n",
    "\n",
    "    return model, train_loss_batches\n",
    "\n",
    "def validate(model, loss_fn, val_loader, device):                 \n",
    "    with torch.no_grad():\n",
    "        batch_val_loss=0\n",
    "        validation_loss = []\n",
    "        gt = []\n",
    "        pr = []\n",
    "        val_mad = []\n",
    "        val_fad = []\n",
    "        model.eval()\n",
    "\n",
    "        for id_b, data in enumerate(val_loader):\n",
    "\n",
    "            # input to encoder input\n",
    "            val_input = data['src']\n",
    "            dec_inpt = data['trg']\n",
    "\n",
    "            # prediction till horizon lenght\n",
    "            for i in range(12):\n",
    "                # getting model prediction\n",
    "                model_output = transformer_model.forward(val_input, dec_inpt)\n",
    "                # appending the predicition to decoder input for next cycle\n",
    "                dec_inp = torch.cat((dec_inp, model_output[:, -1:, :]), 1)\n",
    "\n",
    "            # calculating loss using pairwise distance of all predictions\n",
    "            val_loss = F.pairwise_distance()\n",
    "            batch_val_loss += val_loss.item()\n",
    "        \n",
    "        validation_loss.append(batch_val_loss/len(val_loader))\n",
    "\n",
    "        # calculating mad and fad evaluation metrics\n",
    "        gt = np.concatenate(gt, 0)\n",
    "        pr = np.concatenate(pr, 0)\n",
    "        mad, fad, _ = dataloader.distance_metrics(gt, pr)\n",
    "        val_mad.append(mad)\n",
    "        val_fad.append(fad)\n",
    "\n",
    "        return  mad, fad\n",
    "        # print(\"Epoch {}/{}....Validation mad = {:.4f}, Validation fad = {:.4f}\".format(epoch+1, epochs, mad, fad))\n",
    "\n",
    "def training_loop(model, optimizer, loss_fn, train_loader, val_loader, num_epochs, print_every):\n",
    "    print(\"Starting training\")\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    train_losses, val_mad, val_fad = [], [], []\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        model, train_loss = train_epoch(model, optimizer, loss_fn,train_loader, val_loader, device, print_every)\n",
    "        val_mad, val_fad = validate(model, loss_fn, val_loader, device)\n",
    "        print(f\"Epoch {epoch}/{num_epochs}: \"\n",
    "              f\"Train loss: {sum(train_loss)/len(train_loss):.3f}, \"\n",
    "              f\"Val. mad: {val_mad:.3f}, \"\n",
    "              f\"Val. fad.: {val_fad:.3f}\")\n",
    "        train_losses.extend(train_loss)\n",
    "        val_mad.append(val_mad)\n",
    "        val_fad.append(val_fad)\n",
    "\n",
    "    return model, train_losses, val_mad, val_fad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340 Deep machine learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory estimation.ipynb Cell 19\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mSGD(transformer_model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m1e-4\u001b[39m, momentum\u001b[39m=\u001b[39m\u001b[39m0.9\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m, nesterov\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m first_model, first_train_losses, first_val_mad, first_val_fad \u001b[39m=\u001b[39m training_loop(transformer_model, optimizer, loss_fn, train_loader, val_loader, num_epochs\u001b[39m=\u001b[39;49mnum_epochs, print_every\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "\u001b[1;32m/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340 Deep machine learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory estimation.ipynb Cell 19\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m train_losses, val_mad, val_fad \u001b[39m=\u001b[39m [], [], []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m     model, train_loss \u001b[39m=\u001b[39m train_epoch(model, optimizer, loss_fn,train_loader, val_loader, device, print_every)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m     val_mad, val_fad \u001b[39m=\u001b[39m validate(model, loss_fn, val_loader, device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain loss: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39msum\u001b[39m(train_loss)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(train_loss)\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVal. mad: \u001b[39m\u001b[39m{\u001b[39;00mval_mad\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m           \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVal. fad.: \u001b[39m\u001b[39m{\u001b[39;00mval_fad\u001b[39m:\u001b[39;00m\u001b[39m.3f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340 Deep machine learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory estimation.ipynb Cell 19\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m decoder_input \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mtrg\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m predictions \u001b[39m=\u001b[39m transformer_model\u001b[39m.\u001b[39;49mforward(encoder_input, decoder_input) \u001b[39m# add mask ?\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(predictions, decoder_input)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/trajectory%20estimation.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340 Deep machine learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/model_cbc.py:246\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src, tgt):\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m     \u001b[39m# genarate mask for\u001b[39;00m\n\u001b[0;32m--> 246\u001b[0m     src_mask, tgt_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate_mask(src, tgt)\n\u001b[1;32m    247\u001b[0m     src_embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\n\u001b[1;32m    248\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder_embedding(src)))\n\u001b[1;32m    249\u001b[0m     tgt_embedded \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(\n\u001b[1;32m    250\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpositional_encoding(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder_embedding(tgt)))\n",
      "File \u001b[0;32m~/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340 Deep machine learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/Project/model_cbc.py:240\u001b[0m, in \u001b[0;36mTransformer.generate_mask\u001b[0;34m(self, src, tgt)\u001b[0m\n\u001b[1;32m    237\u001b[0m seq_length \u001b[39m=\u001b[39m tgt\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[1;32m    238\u001b[0m nopeak_mask \u001b[39m=\u001b[39m (\n\u001b[1;32m    239\u001b[0m     \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m torch\u001b[39m.\u001b[39mtriu(torch\u001b[39m.\u001b[39mones(\u001b[39m1\u001b[39m, seq_length, seq_length), diagonal\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mbool()\n\u001b[0;32m--> 240\u001b[0m tgt_mask \u001b[39m=\u001b[39m tgt_mask \u001b[39m&\u001b[39;49m nopeak_mask\n\u001b[1;32m    241\u001b[0m \u001b[39mreturn\u001b[39;00m src_mask, tgt_mask\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (4) must match the size of tensor b (12) at non-singleton dimension 4"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(transformer_model.parameters(), lr=1e-4, momentum=0.9, weight_decay=1e-3, nesterov=True)\n",
    "num_epochs = 1\n",
    "\n",
    "first_model, first_train_losses, first_val_mad, first_val_fad = training_loop(transformer_model, optimizer, loss_fn, train_loader, val_loader, num_epochs=num_epochs, print_every=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
