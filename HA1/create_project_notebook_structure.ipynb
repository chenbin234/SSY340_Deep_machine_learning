{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "20a4d65ee77906a86bc39bc4046a2a36",
     "grade": false,
     "grade_id": "cell-5690119ead85e67e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Checklist for submission\n",
    "\n",
    "It is extremely important to make sure that:\n",
    "\n",
    "1. Everything runs as expected (no bugs when running cells);\n",
    "2. The output from each cell corresponds to its code (don't change any cell's contents without rerunning it afterwards);\n",
    "3. All outputs are present (don't delete any of the outputs);\n",
    "4. Fill in all the places that say `# YOUR CODE HERE`, or \"**Your answer:** (fill in here)\".\n",
    "5. Never copy/paste any notebook cells. Inserting new cells is allowed, but it should not be necessary.\n",
    "6. The notebook contains some hidden metadata which is important during our grading process. **Make sure not to corrupt any of this metadata!** The metadata may for example be corrupted if you copy/paste any notebook cells, or if you perform an unsuccessful git merge / git pull. It may also be pruned completely if using Google Colab, so watch out for this. Searching for \"nbgrader\" when opening the notebook in a text editor should take you to the important metadata entries.\n",
    "7. Although we will try our very best to avoid this, it may happen that bugs are found after an assignment is released, and that we will push an updated version of the assignment to GitHub. If this happens, it is important that you update to the new version, while making sure the notebook metadata is properly updated as well. The safest way to make sure nothing gets messed up is to start from scratch on a clean updated version of the notebook, copy/pasting your code from the cells of the previous version into the cells of the new version.\n",
    "8. If you need to have multiple parallel versions of this notebook, make sure not to move them to another directory.\n",
    "9. Although not forced to work exclusively in the course `conda` environment, you need to make sure that the notebook will run in that environment, i.e. that you have not added any additional dependencies.\n",
    "\n",
    "**FOR HA1, HA2, HA3 ONLY:** Failing to meet any of these requirements might lead to either a subtraction of POEs (at best) or a request for resubmission (at worst).\n",
    "\n",
    "We advise you to perform the following steps before submission to ensure that requirements 1, 2, and 3 are always met: **Restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All). This might require a bit of time, so plan ahead for this (and possibly use a cloud GPU in HA1 and HA2 for this step). Finally press the \"Save and Checkout\" button before handing in, to make sure that all your changes are saved to this .ipynb file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a6bb874a16c1ff767ac0f37ce0491265",
     "grade": false,
     "grade_id": "cell-774c93bf6433de68",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in name of notebook file\n",
    "This might seem silly, but the version check below needs to know the filename of the current notebook, which is not trivial to find out programmatically.\n",
    "\n",
    "You might want to have several parallel versions of the notebook, and it is fine to rename the notebook as long as it stays in the same directory. **However**, if you do rename it, you also need to update its own filename below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fname = \"create_project_notebook_structure.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "879883c2ea755808ffd00aeee5c77a00",
     "grade": false,
     "grade_id": "cell-5676bcf768a7f9be",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Fill in group number and member names (use NAME2 and GROUP only for HA1, HA2 and HA3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME1 = \"Bingcheng Chen\" \n",
    "NAME2 = \"\"\n",
    "GROUP = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "42f960a95815e1aa3ce8132fcec59cd9",
     "grade": false,
     "grade_id": "cell-a15fe781533d9590",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "736e393ef62f60d5e70432726e7209e0",
     "grade": false,
     "grade_id": "cell-2b9c2390ee464c39",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from platform import python_version_tuple\n",
    "assert python_version_tuple()[:2] == ('3','9'), \"You are not running Python 3.9. Make sure to run Python through the course Conda environment.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "15ec4309f1e85f6e17bda73b9b6f48a2",
     "grade": false,
     "grade_id": "cell-4869b45600ce82f8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Check that notebook server has access to all required resources, and that notebook has not moved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a2d199303c73ec86d25177caf39e385f",
     "grade": false,
     "grade_id": "cell-122ac3d9100b8afb",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "nb_dirname = os.path.abspath('')\n",
    "assignment_name = os.path.basename(nb_dirname)\n",
    "assert assignment_name in ['IHA1', 'IHA2', 'HA1', 'HA2', 'HA3'], \\\n",
    "    '[ERROR] The notebook appears to have been moved from its original directory'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f09f40b5350db83232189137c550f0a1",
     "grade": false,
     "grade_id": "cell-2455deee513cd39c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify correct nb_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a78c7227b049bb147e6c363affb6dae8",
     "grade": false,
     "grade_id": "cell-0472e2fd710f1d72",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>if(\"create_project_notebook_structure.ipynb\" != IPython.notebook.notebook_name) { alert(\"You have filled in nb_fname = \\\"create_project_notebook_structure.ipynb\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    display(HTML(r'<script>if(\"{nb_fname}\" != IPython.notebook.notebook_name) {{ alert(\"You have filled in nb_fname = \\\"{nb_fname}\\\", but this does not seem to match the notebook filename \\\"\" + IPython.notebook.notebook_name + \"\\\".\"); }}</script>'.format(nb_fname=nb_fname)))\n",
    "except NameError:\n",
    "    assert False, 'Make sure to fill in the nb_fname variable above!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "98d88d8e8da19693053764f29dcc591d",
     "grade": false,
     "grade_id": "cell-ceacb1adcae4783d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Verify that your notebook is up-to-date and not corrupted in any way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bb700528d4644601c1a8c91ef1d84635",
     "grade": false,
     "grade_id": "cell-f5a59288e11b4aec",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching current notebook against the following URL:\n",
      "http://raw.githubusercontent.com/dml-cth/deep-machine-learning/master/home-assignments/HA1/HA1.ipynb\n",
      "Missing cells: ['cell-00441f4b1107270a', 'cell-012bfbd6ff78577a', 'cell-01ebc4c9c306b985', 'cell-0235e816fc98b0f6', 'cell-051ee24a83af3cf8', 'cell-0b7b0d0139ba368a', 'cell-0f4a5edca490320e', 'cell-111f2b1d28919293', 'cell-12ad4c02e8150588', 'cell-19785940b9624d2c', 'cell-1b1314f2ab1b1d6b', 'cell-1d6ea64bca94a4ef', 'cell-1ee9ebc87fd3358e', 'cell-22d09c8401d84b61', 'cell-24463974ae20a276', 'cell-25f9cc8d17491d0d', 'cell-270f8ec140ddfba3', 'cell-2be321b63232ae01', 'cell-2ea049dea4713494', 'cell-31b81f052b6e681e', 'cell-3918c2cdd9817f7e', 'cell-3a2bedde765aa366', 'cell-3ae2a65188e4ac74', 'cell-3c8d8e5ab949ee35', 'cell-3df999674672de47', 'cell-3ee6d24346a80d85', 'cell-464a08ede00083a4', 'cell-4821dc273028d702', 'cell-48933baad6c5afeb', 'cell-4b4d93eec45833cf', 'cell-4c9de348cd8bc4ff', 'cell-4d42c86687697a67', 'cell-4da6b3b63e3305f0', 'cell-4ed3967e4f6c5f7f', 'cell-506e21ce469b67f5', 'cell-5314d286e79e0377', 'cell-544a73726bebe121', 'cell-5553a56f43c9298a', 'cell-5593ecac89fb79b1', 'cell-56908ee1e60aa411', 'cell-56cb37360051a638', 'cell-594c6039216461e5', 'cell-5afc8b836cbbb30e', 'cell-5ca8fc808d4ee65b', 'cell-5dae528a81d5ff24', 'cell-5dc3e388a41da3ed', 'cell-5e1ddfbfceb4d194', 'cell-64eaa83780f5eac9', 'cell-6509545d160bac4b', 'cell-655d00face15a862', 'cell-674350e34be30d10', 'cell-694a3fbb7f081da8', 'cell-6a1f17f3e517a507', 'cell-6e509c469c8a52af', 'cell-6edb7d7e343ab14b', 'cell-71a8b8de004f6e57', 'cell-76e4aad7fbcf5d05', 'cell-777d7ed9a3cbabd0', 'cell-779d477ffe1ebbf6', 'cell-7cb62a04916a848e', 'cell-7e15f27d83e958ff', 'cell-7e5eb612469340fb', 'cell-7edb12ee397ec817', 'cell-7f3b0dfbd90a14c1', 'cell-8092c3fd452a3245', 'cell-80fa8c89f1b262f1', 'cell-876ca7df88c9311f', 'cell-89ba19509b952af2', 'cell-8bbfa3e11e2dfff9', 'cell-a827c39d9e652e52', 'cell-a97630bf5d85363f', 'cell-ad0efbac33de5a65', 'cell-ad32824e875c79cd', 'cell-ad79e1aa5c4a6185', 'cell-b1861d3a543c6386', 'cell-b38092b08c150e7d', 'cell-b508ede3d760a86b', 'cell-b84dd461d5ddcc8d', 'cell-bfb58ea46c31df0a', 'cell-c06d2ae30ee4c649', 'cell-c0bfc1ac7fadfcc7', 'cell-c4bb694612153106', 'cell-c67bcc4fbec1808e', 'cell-c7dd71a632b5f152', 'cell-c8afb448c67da5f8', 'cell-cb6fc78116ad6b75', 'cell-cbda4b585ad39ddc', 'cell-cc77ac7849f856e1', 'cell-ceaac6be60ce36a9', 'cell-cf811afdac96843b', 'cell-cf9b347fc3ee9255', 'cell-d033937b5a8b9875', 'cell-d519532bb1f957c3', 'cell-d746f9eb61e3ea44', 'cell-dc362abcfef32eae', 'cell-dfd1969f7ce6902a', 'cell-e03f109baaddeb95', 'cell-e3e3990ba39bea67', 'cell-e64508c0fe4fa4f6', 'cell-e79df7472ff5506a', 'cell-e951dcec64dec85d', 'cell-ee79a83a62b70a8f', 'cell-ee9e2aee031325a2', 'cell-f17c882b2a09dee7', 'cell-f2fc166890962bcf', 'cell-f3f79586de42561b', 'cell-f50c3d451530b9a8', 'cell-f7371c24b57c153e', 'cell-f76d1a7f6280af0d', 'cell-f9e1a6a643946cd2', 'cell-fa81712e1e27432a', 'cell-faed8047ef25a60d', 'cell-faf8664f26ff7f4e']\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "[ERROR] Notebook does not seem to be up-to-date. Please follow these instructions to sync with latest GitHub version: https://github.com/dml-cth/deep-machine-learning/blob/master/instructions/YY_keep_git_repo_in_sync.md",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340 Deep machine learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/HA1/create_project_notebook_structure.ipynb Cell 13\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/HA1/create_project_notebook_structure.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sys\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mappend(\u001b[39m'\u001b[39m\u001b[39m..\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/HA1/create_project_notebook_structure.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mha_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m check_notebook_uptodate_and_not_corrupted\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/chenbingcheng/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340%20Deep%20machine%20learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/HA1/create_project_notebook_structure.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)\n",
      "File \u001b[0;32m~/Downloads/STUDY/06_MPICT_Courses/07_semester3/01_SSY340 Deep machine learning/SSY340_Deep_machine_learning/SSY340_Deep_machine_learning/HA1/../ha_utils.py:67\u001b[0m, in \u001b[0;36mcheck_notebook_uptodate_and_not_corrupted\u001b[0;34m(nb_dirname, nb_fname)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(curr_cell_ids \u001b[39m-\u001b[39m ref_cell_ids) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     66\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFound unexpected cells: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39msorted\u001b[39m(curr_cell_ids \u001b[39m-\u001b[39m ref_cell_ids)))\n\u001b[0;32m---> 67\u001b[0m \u001b[39massert\u001b[39;00m ref_cell_ids \u001b[39m==\u001b[39m curr_cell_ids, \\\n\u001b[1;32m     68\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m[ERROR] Notebook does not seem to be up-to-date. Please follow these instructions to sync with latest GitHub version: https://github.com/dml-cth/deep-machine-learning/blob/master/instructions/YY_keep_git_repo_in_sync.md\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     70\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m[SUCCESS] No major notebook mismatch found when comparing to latest GitHub version. (There might be minor updates, but even that is the case, submitting your work based on this notebook version would be acceptable.)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: [ERROR] Notebook does not seem to be up-to-date. Please follow these instructions to sync with latest GitHub version: https://github.com/dml-cth/deep-machine-learning/blob/master/instructions/YY_keep_git_repo_in_sync.md"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from ha_utils import check_notebook_uptodate_and_not_corrupted\n",
    "check_notebook_uptodate_and_not_corrupted(nb_dirname, nb_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the project structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a helper notebook to create the folder structure necessary for HA1.\n",
    "Start looking at `HA1.ipynb` and revisit this notebook when needed.\n",
    "\n",
    "This should be run from the same folder where the `dogs-vs-cats.zip` file you downloaded from Kaggle is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/scipy/__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.25.2\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# For dealing with files we use the built-in python module `Path`\n",
    "# It provides a nice abstraction of the file system, compared to working with strings only.\n",
    "# It also makes your code more portable, i.e. easier to share with someone using another operating system.\n",
    "from pathlib import Path\n",
    "# Some file system operation are not covered by 'Path' and we use 'shutil' for that\n",
    "import shutil\n",
    "\n",
    "# Regular expressions are used to find patterns in strings\n",
    "import re\n",
    "\n",
    "# For splitting the data\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: This script assumes that you have the `dogs-vs-cats.zip` in the same directory as this notebook\n",
    "zip_file = Path.cwd() / \"dogs-vs-cats.zip\"\n",
    "if not zip_file.exists():\n",
    "    raise FileNotFoundError(\"Download and place `{}` in the current directory (`{}`)\"\n",
    "                            .format(zip_file.name, Path.cwd()))\n",
    "\n",
    "# This is a list of all the directories and files this notebook will produce.\n",
    "# If you have run this before, we will delete them and start over from `dogs-vs-cats.zip`\n",
    "# Notice how we use the `map` function to conveniently run `Path(<filename>)` on all strings in our list,\n",
    "# to turn them in portable filepaths.\n",
    "pre_existing_items = map(Path, [\"test1.zip\",\n",
    "                     \"test\",\n",
    "                     \"val\",\n",
    "                     \"train.zip\",\n",
    "                     \"train\",\n",
    "                     \"train_all\",\n",
    "                     \"sampleSubmission.csv\",\n",
    "                     \"small_train\",\n",
    "                     \"small_val\"\n",
    "                     ])\n",
    "\n",
    "for item in pre_existing_items:\n",
    "    if item.exists():\n",
    "        # We need to use different functions for files and directories.\n",
    "        if item.is_dir():\n",
    "            shutil.rmtree(item)\n",
    "        elif item.is_file():\n",
    "            item.unlink()\n",
    "        else:\n",
    "            print(\"Unknown item: {}, remove manually\".format(item))\n",
    "\n",
    "\n",
    "# Depending on your machine the following might take some seconds to run\n",
    "shutil.unpack_archive('dogs-vs-cats.zip')\n",
    "shutil.unpack_archive('test1.zip')\n",
    "shutil.unpack_archive('train.zip')\n",
    "\n",
    "Path(\"test1\").rename(\"test\")\n",
    "Path(\"train\").rename(\"train_all\")\n",
    "\n",
    "\n",
    "# Remove sub zip filess\n",
    "Path(\"test1.zip\").unlink()\n",
    "Path(\"train.zip\").unlink()\n",
    "\n",
    "# Take a look at your current directory. Apart from notebook files (those ending in *.ipynb) you should see\n",
    "# dogs-vs-cats.zip\n",
    "# sampleSubmission.csv\n",
    "# test\n",
    "# train_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll examine the data inside the directory `train_all`.\n",
    "It contains files like this:\n",
    "\n",
    "```\n",
    "<id>.jpg\n",
    "<id>.jpg\n",
    "```\n",
    "where each id is on the from `<label>.<number>`, e.g. `cat.123`.\n",
    "\n",
    "Inside the directory `test` are unlabelled images\n",
    "\n",
    "```\n",
    "<id>.jpg\n",
    "```\n",
    "where each id is a single number.\n",
    "\n",
    "Predictions on these unknown images is what you would submit to participate in the contest.\n",
    "\n",
    "Let's count them separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12500 images of cats.\n",
      "Found 12500 images of dogs.\n",
      "Found 12500 test images\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_all_path = Path.cwd() / \"train_all\"\n",
    "\n",
    "# Get a list of all filenames inside (these will be used for training and validation)\n",
    "# The asterisk '*' is a so called wildcard, i.e. we tell the `glob` method to find any cat/dog images,\n",
    "# regardless of their id.\n",
    "all_cat_filenames = list(train_all_path.glob(\"cat.*.jpg\"))\n",
    "all_dog_filenames = list(train_all_path.glob(\"dog.*.jpg\"))\n",
    "\n",
    "test_path = Path.cwd() / \"test\"\n",
    "all_test_filenames = list(test_path.glob(\"*.jpg\"))\n",
    "print(f\"Found {len(all_cat_filenames)} images of cats.\\nFound {len(all_dog_filenames)} images of dogs.\\nFound {len(all_test_filenames)} test images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create `'small_train'` and `'small_val'` folders for a smaller subset of the original dataset (the assignment asks for 20%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a subset of the entire training dataset (20%)\n",
    "_, few_cat_filenames, _, few_dog_filenames = train_test_split(all_cat_filenames, \n",
    "                                                              all_dog_filenames, \n",
    "                                                              test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smaller dataset will be comprised of:\n",
      "Training:\t1750 cats and 1750 dogs\n",
      "Validation:\t750 cats and 750 dogs\n"
     ]
    }
   ],
   "source": [
    "# Split it into training and validation sets\n",
    "split_ratio_small_dataset = 0.3\n",
    "\n",
    "few_cat_filenames_train, few_cat_filenames_val, few_dog_filenames_train, few_dog_filenames_val = \\\n",
    "train_test_split(few_cat_filenames, \n",
    "              few_dog_filenames, \n",
    "              test_size = split_ratio_small_dataset,\n",
    "              random_state = 2)\n",
    "\n",
    "print(\"The smaller dataset will be comprised of:\")\n",
    "print(f\"Training:\\t{len(few_cat_filenames_train)} cats and {len(few_dog_filenames_train)} dogs\")\n",
    "print(f\"Validation:\\t{len(few_cat_filenames_val)} cats and {len(few_dog_filenames_val)} dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and val directories and subdirectories\n",
    "subdirectories = {\"small_train/cats\": few_cat_filenames_train,\n",
    "                  \"small_train/dogs\": few_dog_filenames_train,\n",
    "                  \"small_val/cats\": few_cat_filenames_val,\n",
    "                  \"small_val/dogs\": few_dog_filenames_val\n",
    "                 }\n",
    "\n",
    "for subdirectory in subdirectories.keys():\n",
    "    subdirectory = Path(subdirectory)\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Put the training and validation data in the respective folders\n",
    "def fill_sub_dir(sub_dir, file_subset):\n",
    "    \"\"\"This function copies files from the `train_all` to a `<sub_dir>`\n",
    "    A more efficient solution would be to use \"symbolic links\" (see https://kb.iu.edu/d/abbe)\n",
    "    but for simplicity hard copies is used instead.\n",
    "    \"\"\"\n",
    "    for file in file_subset:\n",
    "        file_path = Path.cwd() / sub_dir / file.name\n",
    "        shutil.copyfile(file, file_path)\n",
    "        \n",
    "for sub_dir, file_subset in subdirectories.items():\n",
    "    fill_sub_dir(sub_dir, file_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the `val` and `train` folders for the entire dataset. You need to specify the train/val split (to something reasonable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The full dataset will be comprised of:\n",
      "Train:\t11500 cats and 11500 dogs.\n",
      "Val:\t1000 cats and 1000 dogs\n"
     ]
    }
   ],
   "source": [
    "split_ratio_big_dataset = 0.08# Fill in here\n",
    "\n",
    "if split_ratio_big_dataset is None:\n",
    "    raise ValueError(\"`split_ratio_big_dataset` must have a value between 0 and 1.\")\n",
    "\n",
    "# Split it\n",
    "all_cat_filenames_train, all_cat_filenames_val, all_dog_filenames_train, all_dog_filenames_val = \\\n",
    "train_test_split(all_cat_filenames,\n",
    "                 all_dog_filenames,\n",
    "                 test_size=split_ratio_big_dataset,\n",
    "                 random_state=3)\n",
    "\n",
    "print(\"The full dataset will be comprised of:\")\n",
    "print(f\"Train:\\t{len(all_cat_filenames_train)} cats and {len(all_dog_filenames_train)} dogs.\")\n",
    "print(f\"Val:\\t{len(all_cat_filenames_val)} cats and {len(all_dog_filenames_val)} dogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and val directories and subdirectories\n",
    "subdirectories = {\"train/cats\": all_cat_filenames_train,\n",
    "                 \"train/dogs\": all_dog_filenames_train,\n",
    "                 \"val/cats\": all_cat_filenames_val,\n",
    "                 \"val/dogs\": all_dog_filenames_val\n",
    "                 }\n",
    "\n",
    "for subdirectory in subdirectories.keys():\n",
    "    subdirectory = Path(subdirectory)\n",
    "    subdirectory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for sub_dir, file_subset in subdirectories.items():\n",
    "    fill_sub_dir(sub_dir, file_subset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
